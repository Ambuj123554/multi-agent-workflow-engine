ğŸš€ Multi-Agent Workflow Engine

A modular, production-ready AI system that automates problem understanding â†’ research â†’ analysis â†’ final answer generation using multiple collaborating agents. The project includes a FastAPI backend, modern frontend UI, and a fully pluggable LLM abstraction layer (Gemini / Ollama / OpenAI compatible).

â­ Key Features
ğŸ”¹ Multi-Stage AI Pipeline

The system orchestrates four intelligent agents:

Understanding Agent â€“ Interprets the user query and breaks it into actionable tasks.

Research Agent â€“ Gathers information using web-search, tools, or LLM reasoning.

Analysis Agent â€“ Evaluates, summarises, checks correctness, and removes hallucinations.

Final Output Agent â€“ Crafts a clean, human-readable final answer.

â­ Technical Highlights
ğŸ§  Multi-Agent Orchestration

Custom agent pipeline with modular task routing.

Supports automatic validation loops to reduce hallucinations.

Extensible design: easily add tools (web search, RAG, structured output, etc.)

ğŸ’¾ Memory & Persistence

Vector-store powered memory system.

Stores agent conversations and research context.

Reduces redundant API calls and improves long-term reasoning.

âš¡ Backend â€“ FastAPI

Single /api/run endpoint to execute the full pipeline.

Clean architecture and reusable components.

LLM interface layer with plug-and-play models (Gemini/Ollama/OpenAI).

ğŸ¨ Frontend

Modern UI (React / Next.js or whichever you use).

Real-time status updates of each agent.

Beautiful chat-like interface for input and results.

ğŸ³ Docker Support

Fully containerized backend for easy deployment.

Works locally or on cloud (Render, Railway, GCP, etc.)

ğŸ—ï¸ Project Structure
multi-agent-workflow-engine/
â”‚â”€â”€ backend/
â”‚   â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ orchestrator.py
â”‚   â”œâ”€â”€ llm.py
â”‚   â”œâ”€â”€ memory.py
â”‚   â”œâ”€â”€ routers/
â”‚   â”œâ”€â”€ main.py
â”‚
â”‚â”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â””â”€â”€ ...
â”‚
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt

ğŸš€ How It Works
Step 1 â†’ User submits query

The frontend sends a request to /api/run.

Step 2 â†’ Agents collaborate

Each agent performs its role and passes optimized output to the next.

Step 3 â†’ Final refined answer

The system returns a clean, context-aware, non-hallucinated response.

ğŸ§© LLM Integration

Supported:
âœ” Gemini (via Google Generative AI API)
âœ” Ollama (local Llama-3, Mistral, DeepSeek, etc.)
âœ” OpenAI (GPT-4, GPT-4o, etc.)

Swap models by editing just one file: llm.py.

ğŸ“¦ Installation
git clone https://github.com/Ambuj123554/multi-agent-workflow-engine
cd backend
pip install -r requirements.txt
fastapi dev main.py

ğŸ–¥ï¸ Run Frontend
cd frontend
npm install
npm run dev

ğŸ™Œ Contributions

Open to improvements â€” feel free to submit PRs or open issues!

ğŸ“œ License

MIT License
